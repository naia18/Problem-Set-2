{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAIN LIBRARIES to use\n",
    "using Optim\n",
    "using Statistics\n",
    "using ForwardDiff\n",
    "using Plots\n",
    "using LinearAlgebra\n",
    "using CSV\n",
    "using DataFrames\n",
    "using StatsFuns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have the following model:\n",
    "\n",
    "$$\n",
    "\\begin{gather*}\n",
    "    y_t^{*}=\\alpha+\\rho y_{t-1}^{*}+\\beta x_t + \\epsilon_t \\label{eq10}\\tag{10} \\\\\n",
    "    y_t = y_t^{*}+\\upsilon_t\n",
    "\\end{gather*}\n",
    "$$\n",
    "\n",
    "where $\\epsilon_{t}$ and $\\upsilon_{t}$ are independent Gaussian white noise errors. Suppose that $y_{t}^{*}$ is not observed, and instead we observe $y_{t}$. We estimate the equation\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "    y_t=\\alpha+\\rho y_{t-1}+\\beta x_t + \\nu_t \\label{eq11}\\tag{11}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "which will yield a biased and inconsistent estimator, since the errors are not exogenous anymore $\\mathbb{E}[x_t\\nu_t]\\neq 0$.\n",
    "\n",
    "We're told to consider as instruments to deal with this endogeneity: $Z=[1\\,\\, x_t\\,\\, x_{t-1}\\,\\, x_{t-2}]$, lags of $x_t$ which are correlated with $y_{t-1}$ as long as $\\beta\\neq 0$. By assumption, $\\mathbb{E}[x_{t-s}\\nu_t]=0\\quad\\forall s\\geq0$. The functions that we're given to define these lags:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lags (generic function with 2 methods)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function lag(x::Array{Float64,2},p::Int64)\n",
    "\tn,k = size(x)\n",
    "\tlagged_x = [ones(p,k); x[1:n-p,:]]\n",
    "end\n",
    "\n",
    "function lag(x::Array{Float64,1},p::Int64)\n",
    "\tn = size(x,1)\n",
    "\tlagged_x = [ones(p); x[1:n-p]]\n",
    "end\t \n",
    "\n",
    "\n",
    "function  lags(x::Array{Float64,2},p)\n",
    "\tn, k = size(x)\n",
    "\tlagged_x = zeros(eltype(x),n,p*k)\n",
    "\tfor i = 1:p\n",
    "\t\tlagged_x[:,i*k-k+1:i*k] = lag(x,i)\n",
    "\tend\n",
    "    return lagged_x\n",
    "end\t\n",
    "\n",
    "function  lags(x::Array{Float64,1},p)\n",
    "\tn = size(x,1)\n",
    "\tlagged_x = zeros(eltype(x), n,p)\n",
    "\tfor i = 1:p\n",
    "\t\tlagged_x[:,i] = lag(x,i)\n",
    "\tend\n",
    "    return lagged_x\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2.a) Orthogonality conditions\n",
    "Before we had to deal with a particular type of M-Estimator, the ML. In this second exercise, we are treating the other: the GMM estimator. This latter, as opposed to the ML, does not require any knowledge about the distribution function of our data. Instead, its objective function is described as follows:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "     Q_n(\\theta)=-\\frac{1}{2}g_n(\\theta)\\,' \\hat{W}\\, g_n(\\theta),\\quad g_n(\\theta)\\equiv \\frac{1}{n}\\sum_{i=1}^{n}g(x_i;\\theta) \\label{eq12}\\tag{12}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Enclosed in the function $g(x_i;\\theta)$ are some orthogonality conditions that we will try to minimize. In fact, we have seen that endogenous regressors yield biased and inconsistent estimates of the parameters of our models - a linear model, in this case. We will call _instrument_ to the estimators that exogenize our endogenous regressors by exploting the variability of exogenous variables. Let's consider the standard linear model:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "     y = X\\theta + \\nu \\label{eq13}\\tag{13}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "with $X$ an $n\\times k$ matrix of data. In our case, $X=[1\\,\\, y_{t-1}\\,\\, x_t]$: a matrix of size $n\\times 3$. The assumption that the instruments $Z$ are exogenous can be expressed as follows:\n",
    "\n",
    "$$\n",
    "\\begin{gather*}\n",
    "     \\mathbb{E}[z_t \\nu_t]=0 \\\\\n",
    "     \\mathbb{E}[z_tx_t']\\quad \\text{has rank k} \\label{eq14}\\tag{14} \n",
    "\\end{gather*}\n",
    "$$\n",
    "\n",
    "These are the **moment conditions**. The $l$ instruments give us a set of $l$ moments,\n",
    "\n",
    "$$\n",
    "\\begin{gather*}\n",
    "     g_t(\\theta)=Z_t'\\nu_t=Z_t'(y_t-\\alpha-\\rho y_{t-1}-\\beta x_t)\\\\\n",
    "     g_t(\\theta)=\\begin{pmatrix}\n",
    "                    (y_t-\\alpha-\\rho y_{t-1}-\\beta x_t) \\\\\n",
    "                    x_t(y_t-\\alpha-\\rho y_{t-1}-\\beta x_t) \\\\\n",
    "                    x_{t-1}(y_t-\\alpha-\\rho y_{t-1}-\\beta x_t) \\\\\n",
    "                    x_{t-2}(y_t-\\alpha-\\rho y_{t-1}-\\beta x_t)\n",
    "                    \\end{pmatrix} \\label{eq15}\\tag{15} \n",
    "\\end{gather*}\n",
    "$$\n",
    "\n",
    "where $g_t$ is $l\\times 1$ ($4\\times 1$ here). The exogeneity of the instruments means that there are $l$ moment conditions - orthogonality conditions, that will be satisfied at the true value of $\\theta$, which is $\\theta_0$:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "     \\boxed{\\mathbb{E}[g_t(\\theta_0)]=\\mathbb{E}[Z_t'(y_t-X_t\\theta_0)]=0} \\label{eq16}\\tag{16} \n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Each of the $l$ moment equations corresponds to a sample moment, and we write these $l$ sample moments as follows:\n",
    "\n",
    "$$\n",
    "\\begin{gather*}\n",
    "     \\boxed{\\bar{g}(\\theta)=\\frac{1}{n} \\sum_{t=1}^{n}g_t(\\theta)=\n",
    "     \\begin{pmatrix}\n",
    "                    \\frac{1}{n} \\sum_{t=1}^{n}(y_t-\\alpha-\\rho y_{t-1}-\\beta x_t) \\\\\n",
    "                   \\frac{1}{n} \\sum_{t=1}^{n} x_t(y_t-\\alpha-\\rho y_{t-1}-\\beta x_t) \\\\\n",
    "                    \\frac{1}{n} \\sum_{t=1}^{n}x_{t-1}(y_t-\\alpha-\\rho y_{t-1}-\\beta x_t) \\\\\n",
    "                    \\frac{1}{n} \\sum_{t=1}^{n}x_{t-2}(y_t-\\alpha-\\rho y_{t-1}-\\beta x_t)\n",
    "                    \\end{pmatrix}\n",
    "                    =\\frac{1}{n} \\sum_{t=1}^{n}Z_t'(y_t-\\alpha-\\rho y_{t-1}-\\beta x_t)=\\frac{1}{n}Z'\\nu} \\label{eq17}\\tag{17} \n",
    "\\end{gather*}\n",
    "$$\n",
    "\n",
    "Now, the intuition behind GMM is to choose an estimator for $\\theta$, $\\hat{\\theta}$ that sets **these $l=4$ sample moments** as close to zero as possible.\n",
    "\n",
    "But, what if the  equation is overidentified, as it is here?\n",
    "\n",
    "### Identification\n",
    "\n",
    "If the equation is **overidentified, as in our case,**, so that $l > k$, then we have more equations than we do unknowns, and in general it will not be possible to find a $\\hat{\\theta}$ that will set  all $l$ sample  moment conditions to exactly zero. In this case, we take an $l\\times l$ weighting matrix $W$ and use it to construct a quadratic form in the moment conditions.This gives us the GMM objective function:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "     Q_n(\\theta)= n\\bar{g}(\\theta)'W\\bar{g}(\\theta) \\label{eq18}\\tag{18} \n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Here, $\\bar{g(\\theta)}$ is $m_n(\\theta)$ on our course notes. I only use $g$ not to confuse it with the $m$ in the M-Estimators.  A GMM estimator for $\\theta$ is the $\\hat{\\theta}$ that minimizes $Q_n(\\theta)$:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "     \\frac{\\partial Q_n(\\theta)}{\\partial \\theta}=0 \\label{eq19}\\tag{19} \n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2. b) Compute $\\hat{\\theta}$ using two step GMM.\n",
    "\n",
    "The two step GMM estimator:\n",
    "\n",
    "1. We will first set the weight matrix to some positive definite matrix: $W=I$. Obtain the GMM estimator that minimizes $Q_n(\\theta)=\\bar{g(\\theta)}'W\\bar{g(\\theta)}$.\n",
    "2. Based on this initial estimate $\\hat{\\theta}$, we compute its moment contributions $g_t(\\hat{\\theta})$. Compute an estimate of $\\Omega_{\\infty}$ based on the moment contributions, say $\\hat{\\Omega}^{-1}$. The exact way to do this will depend upon the assumptions of the model. Given the estimate, compute the efficient GMM estimator which minimizes: $Q_n(\\theta)=\\bar{g(\\theta)}'\\hat{\\Omega}^{-1}\\bar{g(\\theta)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fminunc (generic function with 1 method)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ------------------- MAIN FUNCTIONS ------------------------- \n",
    "\n",
    "# what is the best moment to use???\n",
    "\n",
    "# moment condition\n",
    "function GIVmoments(theta, data)\n",
    "    data = [data lags(data,2)]\n",
    "    data = data[3:end,:] # get rid of missings\n",
    "    n = size(data,1)\n",
    "    y = data[:,1]\n",
    "    ylag = data[:,2]\n",
    "    x = data[:,3]\n",
    "    xlag = data[:,6]\n",
    "    xlag2 = data[:,9]\n",
    "    X = [ones(n,1) ylag x]\n",
    "    e = y - X*theta\n",
    "    Z = [ones(n,1) x xlag xlag2]\n",
    "    m = e.*Z\n",
    "    return m\n",
    "end\n",
    "\n",
    "function gmm(moments, theta, data, weight)\n",
    "    # average moments\n",
    "    m = theta -> vec(mean(moments(theta,data),dims=1)) # 1Xg   \n",
    "    # GMM objective function\n",
    "    obj = theta -> ((m(theta))'weight*m(theta))\n",
    "    # Minimization\n",
    "    thetahat, objvalue, converged = fminunc(obj, theta)\n",
    "    # Derivative of average moments\n",
    "    D = (ForwardDiff.jacobian(m, vec(thetahat)))' \n",
    "    # moment contributions at estimate\n",
    "    mc_thetahat = moments(thetahat,data)\n",
    "    return thetahat, objvalue, D, mc_thetahat, converged\n",
    "end\n",
    "\n",
    "# Unconstrained minimization problem    \n",
    "function fminunc(obj, x; tol = 1e-08)\n",
    "    results = Optim.optimize(obj, x, LBFGS(), \n",
    "                            Optim.Options(\n",
    "                            g_tol = tol,\n",
    "                            x_tol=tol,\n",
    "                            f_tol=tol))\n",
    "    return results.minimizer, results.minimum, Optim.converged(results)\n",
    "    #xopt, objvalue, flag = fmincon(obj, x, tol=tol)\n",
    "    #return xopt, objvalue, flag\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A brief description of what the provided code does\n",
    "\n",
    "First we generate data for some particular values of the parameters: $\\theta=[\\alpha_0,\\rho_0,\\beta_0]=[0.0,0.9,1.0]$. The data will be generated using the lag functions defined above. In this part of the code, the main aim is to generate the dependent variable that **we do not observe**, which is $y_t^{*}=\\alpha+\\rho y_{t-1}^{*}+\\beta x_t+\\epsilon_t$. We are assuming that both the exogenous regressor and the error term are normally distributed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------- SET UP ------------------------- \n",
    "\n",
    "n = 1000\n",
    "x = randn(n) # an exogenous regressor\n",
    "e = randn(n) # the error term\n",
    "ystar = zeros(n)\n",
    "alpha_0 = 0.0\n",
    "rho_0 = 0.9\n",
    "beta_0 = 1.0\n",
    "theta = [alpha_0 rho_0 beta_0]\n",
    "# generate the unobserved dependent variable\n",
    "for t = 2:n\n",
    "  ystar[t] = theta[1] + theta[2]*ystar[t-1] + theta[3]*x[t] + e[t]\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once $y_t^{*}$ is defined, we construct the observed variable $y_t$, assuming once again that the error $\\upsilon_t$ is normally distributed with a variance of $\\sigma^2$.\n",
    "\n",
    "Then, we construct a matrix called _data_ which will hold the values of $y_t$, $y_{t-1}$ and $x_t$ (first observation omitted, because of the lag)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the observed dependent variable by adding measurement error\n",
    "sig = 1\n",
    "y = ystar + sig*randn(n);\n",
    "ylag = lag(y,1);\n",
    "data = [y ylag x];\n",
    "data = data[2:end,:]; # drop first observation, missing due to lag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"data-frame\"><thead><tr><th></th><th>Thetas</th><th>Values</th></tr><tr><th></th><th>String</th><th>Array…</th></tr></thead><tbody><p>3 rows × 2 columns</p><tr><th>1</th><td>True value θ_0</td><td>[0.0 0.9 1.0]</td></tr><tr><th>2</th><td>First estimation θ1_hat</td><td>[0.0125454, 0.947209, 1.00944]</td></tr><tr><th>3</th><td>Second estimation θ2_hat</td><td>[0.0113542, 0.948974, 1.00857]</td></tr></tbody></table>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|cc}\n",
       "\t& Thetas & Values\\\\\n",
       "\t\\hline\n",
       "\t& String & Array…\\\\\n",
       "\t\\hline\n",
       "\t1 & True value θ\\_0 & [0.0 0.9 1.0] \\\\\n",
       "\t2 & First estimation θ1\\_hat & [0.0125454, 0.947209, 1.00944] \\\\\n",
       "\t3 & Second estimation θ2\\_hat & [0.0113542, 0.948974, 1.00857] \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "\u001b[1m3×2 DataFrame\u001b[0m\n",
       "\u001b[1m Row \u001b[0m│\u001b[1m Thetas                   \u001b[0m\u001b[1m Values                         \u001b[0m\n",
       "\u001b[1m     \u001b[0m│\u001b[90m String                   \u001b[0m\u001b[90m Array…                         \u001b[0m\n",
       "─────┼──────────────────────────────────────────────────────────\n",
       "   1 │ True value θ_0            [0.0 0.9 1.0]\n",
       "   2 │ First estimation θ1_hat   [0.0125454, 0.947209, 1.00944]\n",
       "   3 │ Second estimation θ2_hat  [0.0113542, 0.948974, 1.00857]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ------------------- GMM TWO-STEP ESTIMATION ------------------------- \n",
    "\n",
    "theta_trial = [1.0, 0.5, 0.5]                     # Trial value of parameter estimators\n",
    "moments = (theta,data) -> GIVmoments(theta,data)  # Generate the moments function to send as an argument for the gmm()\n",
    "\n",
    "# -------------- FIRST ESTIMATION ---------------\n",
    "thetahat1, junk, junk, ms, junk = gmm(moments, theta_trial, data, I(4));\n",
    "W = inv(cov(ms)); \n",
    "# use  thetahat1 to re-estimate by defining a specific weighting matrix\n",
    "\n",
    "# -------------- SECOND ESTIMATION --------------\n",
    "thetahat, junk, junk, ms, junk = gmm(moments, thetahat1, data, W);\n",
    "\n",
    "# Compare the estimators\n",
    "df = DataFrame(Thetas = [\"True value θ_0\", \"First estimation θ1_hat\", \"Second estimation θ2_hat\"], Values = [theta, thetahat1, thetahat])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, both the first estimated $\\theta$ and the second round one are quite close, which means that weighing all the moments equally was satisfactory. Thus, for the sake of efficiency in this particular problem, we could stay with the first estimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.6.0",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
